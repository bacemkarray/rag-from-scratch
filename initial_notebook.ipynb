{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e29713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d40f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539c4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2640dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a537049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bkarr\\projects\\rag-from-scratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\bkarr\\projects\\rag-from-scratch\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bkarr\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Configure sentence transformer embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92016fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae3eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
    "    try:\n",
    "        # Read the document\n",
    "        content = read_document(file_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text(content)\n",
    "\n",
    "        # Prepare metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
    "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        return ids, chunks, metadatas\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3963cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(collection, ids, texts, metadatas):\n",
    "    \"\"\"Add documents to collection in batches\"\"\"\n",
    "    if not texts:\n",
    "        return\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        end_idx = min(i + batch_size, len(texts))\n",
    "        collection.add(\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path: str):\n",
    "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        ids, texts, metadatas = process_document(file_path)\n",
    "        add_to_collection(collection, ids, texts, metadatas)\n",
    "        print(f\"Added {len(texts)} chunks to collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b56197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and add documents from a folder\n",
    "folder_path = \"docs/\"\n",
    "process_and_add_documents(collection, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3515c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(\"\\nSearch Results:\\n\" + \"-\" * 50)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i + 1}\")\n",
    "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Content: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c42ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\"\"], n_results=3)\n",
    "print_search_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
