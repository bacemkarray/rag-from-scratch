{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e29713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d40f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "539c4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2640dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a537049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sentence transformer embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92016fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae3eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
    "    try:\n",
    "        # Read the document\n",
    "        content = read_document(file_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text(content)\n",
    "\n",
    "        # Prepare metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
    "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        return ids, chunks, metadatas\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3963cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(collection, ids, texts, metadatas):\n",
    "    \"\"\"Add documents to collection in batches\"\"\"\n",
    "    if not texts:\n",
    "        return\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        end_idx = min(i + batch_size, len(texts))\n",
    "        collection.add(\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path: str):\n",
    "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        ids, texts, metadatas = process_document(file_path)\n",
    "        add_to_collection(collection, ids, texts, metadatas)\n",
    "        print(f\"Added {len(texts)} chunks to collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b56197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and add documents from a folder\n",
    "folder_path = \"docs/\"\n",
    "process_and_add_documents(collection, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7dddaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, query: str, n_results: int = 2):\n",
    "    \"\"\"Perform semantic search on the collection\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def get_context_with_sources(results):\n",
    "    \"\"\"Extract context and source information from search results\"\"\"\n",
    "    # Combine document chunks into a single context\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "\n",
    "    # Format sources with metadata\n",
    "    sources = [\n",
    "        f\"{meta['source']} (chunk {meta['chunk']})\" \n",
    "        for meta in results['metadatas'][0]\n",
    "    ]\n",
    "\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3515c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(\"\\nSearch Results:\\n\" + \"-\" * 50)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i + 1}\")\n",
    "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Content: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c42ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\"\"], n_results=3)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2fee332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads variables from .env into os.environ\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6fea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context, conversation_history, query):\n",
    "  prompt = f\"\"\"Based on the following context and conversation history, please provide a relevant and contextual response.\n",
    "    If the answer cannot be derived from the context, only use the conversation history or say \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate response function with conversation history also passed for Chatbot Memory\n",
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using OpenAI with conversation history\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "    # print(prompt)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4027e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(collection, query: str, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\n",
    "    # Get relevant chunks\n",
    "    results = semantic_search(collection, query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    return response, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"my name is bacem. what is my name?\"\n",
    "response, sources = rag_query(collection, query)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nAnswer:\", response)\n",
    "print(\"\\nSources used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e48552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# In-memory conversation store\n",
    "conversations = {}\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a new conversation session\"\"\"\n",
    "    session_id = str(uuid.uuid4())\n",
    "    conversations[session_id] = []\n",
    "    return session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(session_id: str, role: str, content: str):\n",
    "    \"\"\"Add a message to the conversation history\"\"\"\n",
    "    if session_id not in conversations:\n",
    "        conversations[session_id] = []\n",
    "\n",
    "    conversations[session_id].append({\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "def get_conversation_history(session_id: str, max_messages: int = None):\n",
    "    \"\"\"Get conversation history for a session\"\"\"\n",
    "    if session_id not in conversations:\n",
    "        return []\n",
    "\n",
    "    history = conversations[session_id]\n",
    "    if max_messages:\n",
    "        history = history[-max_messages:]\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209bee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_history_for_prompt(session_id: str, max_messages: int = 5):\n",
    "    \"\"\"Format conversation history for inclusion in prompts\"\"\"\n",
    "    history = get_conversation_history(session_id, max_messages)\n",
    "    formatted_history = \"\"\n",
    "\n",
    "    for msg in history:\n",
    "        role = \"Human\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        formatted_history += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "    return formatted_history.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize_query(query: str, conversation_history: str, client: OpenAI):\n",
    "    \"\"\"Convert follow-up questions into standalone queries\"\"\"\n",
    "    contextualize_prompt = \"\"\"Given a chat history and the latest user question \n",
    "    which might reference context in the chat history, formulate a standalone \n",
    "    question which can be understood without the chat history. Do NOT answer \n",
    "    the question, just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": contextualize_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Chat history:\\n{conversation_history}\\n\\nQuestion:\\n{query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error contextualizing query: {str(e)}\")\n",
    "        return query  # Fallback to original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_rag_query(\n",
    "    collection,\n",
    "    query: str,\n",
    "    session_id: str,\n",
    "    n_chunks: int = 3\n",
    "):\n",
    "    \"\"\"Perform RAG query with conversation history\"\"\"\n",
    "    # Get conversation history\n",
    "    conversation_history = format_history_for_prompt(session_id)\n",
    "\n",
    "    # Handle follo up questions\n",
    "    query = contextualize_query(query, conversation_history, client)\n",
    "    print(\"Contextualized Query:\", query)\n",
    "\n",
    "    # Get relevant chunks\n",
    "    context, sources = get_context_with_sources(\n",
    "        semantic_search(collection, query, n_chunks)\n",
    "    )\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Sources:\", sources)\n",
    "\n",
    "\n",
    "    response = generate_response(query, context, conversation_history)\n",
    "\n",
    "    # Add to conversation history\n",
    "    add_message(session_id, \"user\", query)\n",
    "    add_message(session_id, \"assistant\", response)\n",
    "\n",
    "    return response, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conversation session\n",
    "session_id = create_session()\n",
    "\n",
    "# First question\n",
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "response, sources = conversational_rag_query(\n",
    "            collection,\n",
    "            query,\n",
    "            session_id\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where is it located?\"\n",
    "response, sources = conversational_rag_query(\n",
    "            collection,\n",
    "            query,\n",
    "            session_id\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
