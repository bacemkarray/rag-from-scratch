{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e29713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d40f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "539c4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2640dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a537049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sentence transformer embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92016fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae3eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
    "    try:\n",
    "        # Read the document\n",
    "        content = read_document(file_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text(content)\n",
    "\n",
    "        # Prepare metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
    "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        return ids, chunks, metadatas\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3963cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(collection, ids, texts, metadatas):\n",
    "    \"\"\"Add documents to collection in batches\"\"\"\n",
    "    if not texts:\n",
    "        return\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        end_idx = min(i + batch_size, len(texts))\n",
    "        collection.add(\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path: str):\n",
    "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        ids, texts, metadatas = process_document(file_path)\n",
    "        add_to_collection(collection, ids, texts, metadatas)\n",
    "        print(f\"Added {len(texts)} chunks to collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b56197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Swipe+Right+for+Foot+Wipe+-+Part+2+-+Wiping+Away+Boundaries+-+themaneloco.pdf...\n",
      "Added 430 chunks to collection\n",
      "Processing The+Only+Fan+-+Part+2+-+Rent+Day+-+themaneloco.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 309 chunks to collection\n",
      "Processing Up+in+the+Air+-+themaneloco.pdf...\n",
      "Added 863 chunks to collection\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and add documents from a folder\n",
    "folder_path = \"docs/\"\n",
    "process_and_add_documents(collection, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7dddaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, query: str, n_results: int = 2):\n",
    "    \"\"\"Perform semantic search on the collection\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def get_context_with_sources(results):\n",
    "    \"\"\"Extract context and source information from search results\"\"\"\n",
    "    # Combine document chunks into a single context\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "\n",
    "    # Format sources with metadata\n",
    "    sources = [\n",
    "        f\"{meta['source']} (chunk {meta['chunk']})\" \n",
    "        for meta in results['metadatas'][0]\n",
    "    ]\n",
    "\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3515c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(\"\\nSearch Results:\\n\" + \"-\" * 50)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i + 1}\")\n",
    "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Content: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c42ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 1\n",
      "Source: Swipe+Right+for+Foot+Wipe+-+Part+2+-+Wiping+Away+Boundaries+-+themaneloco.pdf, Chunk 332\n",
      "Distance: 1.0437136888504028\n",
      "Content: However, I felt compelled, as their makeshift foot  rest, to do my job to the best of my ability and not interrupt their pleasurable evening   Wiping Away Boundaries - Copyright © 2022 themaneloco     59   together. All the while, it was growing increasingly humid beneath their dresses, and  a deep, throbbing pulsation arose in my skin wherever their sandals were resting at  that moment.\n",
      "\n",
      "\n",
      "Result 2\n",
      "Source: The+Only+Fan+-+Part+2+-+Rent+Day+-+themaneloco.pdf, Chunk 55\n",
      "Distance: 1.0799521207809448\n",
      "Content: Every permitted occasion at her  gorgeous feet came with the added stipulation of making Olivia even more money. There was never a simple relaxing foot rub between flatmates, or similar intimate trysts  between dominant and submissive. “Come on, hurry up, you silly slave girl,” she said impatiently. “I actually want to  eat it before it melts.”  “Umm, yes, umm, Goddess Olivia, sorry,” I said, while fidgeting awkwardly,  before heading to the kitchen and collecting a spoon.\n",
      "\n",
      "\n",
      "Result 3\n",
      "Source: Swipe+Right+for+Foot+Wipe+-+Part+2+-+Wiping+Away+Boundaries+-+themaneloco.pdf, Chunk 395\n",
      "Distance: 1.1033869981765747\n",
      "Content: “You want to come up onto the  bed and love me properly? Just like the way you love my feet?”   Wiping Away Boundaries - Copyright © 2022 themaneloco     70   I had to avert my eyes, an intense feeling of shyness washing over me. Just the  thought of doing stuff with Arpita, other than with her feet, was truly terrifying. I’d felt  weird enough the previous day when she’d brought me to orgasm with her toe, but  still, this was entirely different.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(query_texts=[\"\"], n_results=3)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2fee332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads variables from .env into os.environ\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6fea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context: str, conversation_history: str, query: str):\n",
    "    \"\"\"Generate a prompt combining context, history, and query\"\"\"\n",
    "    prompt = f\"\"\"Based on the following context and conversation history, \n",
    "    please provide a relevant and contextual response. If the answer cannot \n",
    "    be derived from the context, only use the conversation history or say \n",
    "    \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d1b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using OpenAI with conversation history\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # or gpt-3.5-turbo for lower cost\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # Lower temperature for more focused responses\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4027e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(collection, query: str, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\n",
    "    # Get relevant chunks\n",
    "    results = semantic_search(collection, query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    return response, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbf1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'semantic_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWho was dominated on the airplane?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response, sources = \u001b[43mrag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuery:\u001b[39m\u001b[33m\"\u001b[39m, query)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrag_query\u001b[39m\u001b[34m(collection, query, n_chunks)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get relevant chunks\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43msemantic_search\u001b[49m(collection, query, n_chunks)\n\u001b[32m      5\u001b[39m context, sources = get_context_with_sources(results)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'semantic_search' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"\"\n",
    "response, sources = rag_query(collection, query)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nAnswer:\", response)\n",
    "print(\"\\nSources used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
